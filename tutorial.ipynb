{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Packages needed to access data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe4829c210d22c33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieve QUBO matrix of an instance\n",
    "- How to retrieve the QUBO matrix of instance `id`?\n",
    "- Which problem class is `id` instance of?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4fbd8962ac1fad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select instance with id = 5\n",
    "# By changing id, we can retrieve the QUBO matrix of other instances\n",
    "id = 5\n",
    "Q = pd.read_csv(f'data/qubo_dataset/{id}.csv', index_col=[0]).to_numpy()\n",
    "\n",
    "# The characteristics of the instance are stored in file map_index_to_instance.csv\n",
    "map_df = pd.read_csv('data/map_index_to_instance.csv', index_col=[0])\n",
    "print(map_df.loc[id])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5728f15f2041110"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solve an instance with Quantum Annealing\n",
    "We want to solve the instance 5 with quantum annealing.\n",
    "To do this, you need the `ocean-sdk`.\n",
    "To do so, we proceed with the following steps:\n",
    "1. Transform matrix `Q` into a Binary Quadratic Problem (BQM)\n",
    "2. Embed the BQM onto the quantum annealer\n",
    "3. Sample solutions\n",
    "\n",
    "We need the following packages:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e59333bfb2cd36c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dimod\n",
    "from dwave.system import DWaveSampler, FixedEmbeddingComposite\n",
    "from ast import literal_eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41669aae760a9f34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the case we want to use a precomputed embedding, as one of those in the `embeddings` folder, we rely on these helper functions we have defined."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e68cd474b5fd269"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dwave.system import FixedEmbeddingComposite\n",
    "from ast import literal_eval\n",
    "\n",
    "# Return a dictionary containing the embedding of the instance.\n",
    "def extract_embedding(id):\n",
    "    dataframe = pd.read_csv(f'data/embeddings/{id}.csv', index_col=0, header=None, dtype=object)\n",
    "    dataframe[1] = dataframe[1].apply(lambda x: literal_eval(x))\n",
    "    dictionary = dataframe.to_dict()\n",
    "    return dictionary[1]\n",
    "\n",
    "embedding = extract_embedding(id)\n",
    "print(embedding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb0f5d0ea6caa732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def array_to_dict(a):\n",
    "    n = len(a)\n",
    "    d = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            if a[i, j] != 0:\n",
    "                d[(i, j)] = a[i, j]\n",
    "    return d\n",
    "\n",
    "Q_dict = array_to_dict(Q)\n",
    "print(Q_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a3ce0c95f5abb66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Transform the QUBO matrix into BQM format"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34f5ead29d25ca2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bqm = dimod.binary.BinaryQuadraticModel.from_qubo(Q_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5499ee2bf55cb161"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Embed the problem on DWave Advantage6.4. You need a token to access the Quantum Annealer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd4d985cd8b8d351"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qpu = DWaveSampler(token='d-wave_access_token',\n",
    "                   solver={'name': 'Advantage_system6.4'})\n",
    "\n",
    "sampler = FixedEmbeddingComposite(qpu, embedding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af1e155432807f41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Solve the instances with quantum annealing. We define the number of samples `num_reads` and the `annealing_time` (microseconds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64f656cf208e8cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_reads = 50\n",
    "annealing_time = 20\n",
    "sampleset = sampler.sample(bqm,\n",
    "                           num_reads=num_reads,\n",
    "                           annealing_time=annealing_time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa27a8550bb78be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can solve the QUBO problem also with another solver, as Simulated Annealing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "499489615a1a0c65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dwave.samplers import SimulatedAnnealingSampler\n",
    "\n",
    "num_reads = 50\n",
    "sampler = SimulatedAnnealingSampler()\n",
    "sampleset = sampler.sample(bqm, num_reads=num_reads)\n",
    "\n",
    "# Sample with the best value of the cost function\n",
    "first = sampleset.first\n",
    "print(first)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9692b92bc442c9a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Meta-Learning Dataset "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3bf1afc558a63d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metalearning_df = pd.read_csv('data/metalearning_dataset.csv', \n",
    "                              index_col=[0], header=[0,1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48d015667a53b0b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results of the Meta-models\n",
    "We access to the results of the meta-models trained over the small instances.\n",
    "Change `size_instances` to `'large'` to see the results for the large instances.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db98b884dd6e1643"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "size_instances = 'small'\n",
    "metamodels_results_df = pd.read_csv(f'data/metamodels_results/{size_instances}/metamodels_results.csv',\n",
    "                                    index_col=[0,1],\n",
    "                                    header=[0,1,2])\n",
    "metamodels_results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "899b9f807836238"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and testing a meta-model\n",
    "We give an example on how to train and test a meta-model with the Meta-Learning dataset.\n",
    "\n",
    "We train an AdaBoost meta-model with the `LogIsing` domain of the small instances, to predict the label `Optimal` for the Quantum Annealing (`QA`). We use balanced accuracy to evaluate the performance of the meta-model.\n",
    "Some features of this domain are not defined for some instances.\n",
    "In our work, we substitute the `nan` values of a certain feature `f` with the mean value of `f`. We do it also here. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b74edb719c96f6e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Features in the domain LogIsing\n",
    "domain = 'LogIsing'\n",
    "X = metalearning_df.loc[range(246), domain]\n",
    "\n",
    "# Target data\n",
    "target = ('Optimal', 'QA')\n",
    "y = metalearning_df.loc[range(246), target]\n",
    "\n",
    "# Replace not defined value in X with the mean values of the features\n",
    "a = np.isinf(X)\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Features are scaled in the range [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "for col in X.columns:\n",
    "    transformed_col = scaler.fit_transform(pd.DataFrame(X.loc[:,col]))\n",
    "    X.loc[:,col] = transformed_col"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c76c97a40929aca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate two data splits: one contains the data used for the training (67% of the instances), the other contains the data used to test the meta-model (33% of the instances). In this example, we generate the stratified splits over the problem class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35995ec3b5ce9355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.33\n",
    "problem = ('instance_id', 'problem')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=test_size, stratify= metalearning_df.loc[range(246), problem])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94136a1b5db3d0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train the model and compute its balanced_accuracy on the test split."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99fdf81ed4b22094"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training of the meta-model\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Testing of the meta-model\n",
    "y_predicted = model.predict(X_test)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_predicted)\n",
    "print(f'Balanced Accuracy of the meta-model: {balanced_accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c627fe353f87e2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute the importance of the features of the metamodel with permutation feature importance.\n",
    "Each feature is shuffled for `n_repeats = 100` times and the loss in the balanced accuracy is computed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f134c4915c674d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "permutation_importances = permutation_importance(model, X_test, y_test,\n",
    "                                                scoring='balanced_accuracy', n_repeats=100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8be24f10d28ec59b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importances = {}\n",
    "for feature, importance  in zip(X.columns, permutation_importances.importances_mean):\n",
    "    feature_importances[feature] = importance\n",
    "    \n",
    "# We sort the feature from the most important one to the least important one\n",
    "print(f'FEATURE IMPORTANCES')\n",
    "for key in sorted(feature_importances, key=feature_importances.get, reverse=True):\n",
    "    print(f'{key}: {feature_importances[key]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "650b0a921f47e17b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Importance\n",
    "We access to the results of the feature importance of the meta-models trained over the small instances.\n",
    "Change `size_instances` to `'large'` to see the results for the large instances."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d484f4719022ee2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "size_instances = 'small'\n",
    "feature_importance_df = pd.read_csv(f'data/feature_importance/{size_instances}/feature_importance.csv',\n",
    "                                    index_col=[0,1],\n",
    "                                    header=[0,1,2])\n",
    "feature_importance_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38dd433b2b917be1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8bdaf46df9cf7aa3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
